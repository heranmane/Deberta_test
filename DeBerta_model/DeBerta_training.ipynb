{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import transformers \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import dynalab\n",
    "from dynalab.handler.base_handler import BaseDynaHandler, ROOTPATH\n",
    "from dynalab.tasks.hs import TaskIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `oci.config` not found.\n"
     ]
    }
   ],
   "source": [
    ">>> from oci.config import from_file\n",
    "# >>> config = from_file()\n",
    "??oci.config\n",
    "# # Using a different profile from the default location\n",
    "# >>> config = from_file(profile_name=\"integ-beta\")\n",
    "\n",
    "# # Using the default profile from a different file\n",
    "# >>> config = from_file(file_location=\"~/.oci/config.prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: use the following line to import modules from your repo\n",
    "sys.path.append(ROOTPATH)\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??AutoModelForSequenceClassification.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaForSequenceClassification(\n",
       "  (deberta): DebertaModel(\n",
       "    (embeddings): DebertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
       "      (LayerNorm): DebertaLayerNorm()\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (1): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (2): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (3): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (4): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (5): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (6): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (7): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (8): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (9): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (10): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (11): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(1024, 768)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained('.')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('.')\n",
    "tokenizer = AutoTokenizer.from_pretrained('.')\n",
    "\n",
    "model.to(\"cpu\")\n",
    "# model.eval()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `DebertaTokenizer` not found.\n"
     ]
    }
   ],
   "source": [
    "??DebertaTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>state</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>racist</th>\n",
       "      <th>social_justice</th>\n",
       "      <th>food</th>\n",
       "      <th>exclude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.010000e+18</td>\n",
       "      <td>ny</td>\n",
       "      <td>we gotta get back in the lab niggas done stepp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.890000e+17</td>\n",
       "      <td>tx</td>\n",
       "      <td>just another example about how black womens pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.710000e+17</td>\n",
       "      <td>mi</td>\n",
       "      <td>cause be killin these niggas out here</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.020000e+18</td>\n",
       "      <td>ut</td>\n",
       "      <td>wheres our candy nigga in hawaii lmfao aint bu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.010000e+18</td>\n",
       "      <td>ga</td>\n",
       "      <td>nah real niggas woke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>9.180000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nigga paying year in child care not factoring ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>8.850000e+17</td>\n",
       "      <td>tx</td>\n",
       "      <td>ion know if you pull up there my nigga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>8.900000e+17</td>\n",
       "      <td>tx</td>\n",
       "      <td>follow the leader ass swag biting ass niggas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>8.920000e+17</td>\n",
       "      <td>fl</td>\n",
       "      <td>anyone wonder if it about the proposed sale of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9799</th>\n",
       "      <td>9.450000e+17</td>\n",
       "      <td>fl</td>\n",
       "      <td>christmas so diffferent now that nigga grown s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tweet_id state                                      cleaned_tweet  \\\n",
       "0     1.010000e+18    ny  we gotta get back in the lab niggas done stepp...   \n",
       "1     9.890000e+17    tx  just another example about how black womens pa...   \n",
       "2     9.710000e+17    mi              cause be killin these niggas out here   \n",
       "3     1.020000e+18    ut  wheres our candy nigga in hawaii lmfao aint bu...   \n",
       "4     1.010000e+18    ga                               nah real niggas woke   \n",
       "...            ...   ...                                                ...   \n",
       "9795  9.180000e+17   NaN  nigga paying year in child care not factoring ...   \n",
       "9796  8.850000e+17    tx             ion know if you pull up there my nigga   \n",
       "9797  8.900000e+17    tx       follow the leader ass swag biting ass niggas   \n",
       "9798  8.920000e+17    fl  anyone wonder if it about the proposed sale of...   \n",
       "9799  9.450000e+17    fl  christmas so diffferent now that nigga grown s...   \n",
       "\n",
       "      positive neutral  negative  racist social_justice  food  exclude  \n",
       "0          NaN       1       NaN       0            NaN   NaN      NaN  \n",
       "1          NaN     NaN       NaN       0              1   NaN      NaN  \n",
       "2          NaN     NaN       1.0       0            NaN   NaN      NaN  \n",
       "3          NaN       1       NaN       0            NaN   NaN      NaN  \n",
       "4          NaN       1       NaN       0            NaN   NaN      NaN  \n",
       "...        ...     ...       ...     ...            ...   ...      ...  \n",
       "9795       NaN       1       NaN       0            NaN   NaN      NaN  \n",
       "9796       NaN       1       NaN       0            NaN   NaN      NaN  \n",
       "9797       NaN     NaN       1.0       0            NaN   NaN      NaN  \n",
       "9798       NaN       1       NaN       0            NaN   NaN      NaN  \n",
       "9799       NaN       1       NaN       0            NaN   NaN      NaN  \n",
       "\n",
       "[9800 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "df = pd.read_csv(\"data/training_topic_modv2.csv\", error_bad_lines=False)\n",
    "# df.shape\n",
    "# sample of data \n",
    "# sample_df = df.sample(n=5000)\n",
    "# sample_df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williammdavis/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9800, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = df[['cleaned_tweet', 'racist']]\n",
    "sample_df['racist']= sample_df['racist'].fillna(0)\n",
    "# s_df = sample_df.sample(n=1000)\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hate_df = sample_df[sample_df['racist']==1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hate_df['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_hate_df = sample_df[sample_df['racist']==0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(hate_df), len(no_hate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.concat([hate_df,no_hate_df])\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=sample_df.sample(frac=train_size,random_state=100).reset_index(drop=True)\n",
    "test_dataset=sample_df.drop(train_dataset.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (9800, 2)\n",
      "TRAIN Dataset: (7840, 2)\n",
      "TEST Dataset: (1960, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"FULL Dataset: {}\".format(sample_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_tweet    6993\n",
       "racist           6993\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raicis_df = (train_dataset[train_dataset['racist']==0].count())\n",
    "raicis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = sample_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sample_df['cleaned_tweet'].values\n",
    "annotations = sample_df['racist'].astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dummy = [tokenizer(i) for i in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 81)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lengths of tokens\n",
    "a = [len(i.input_ids) for i in batch_dummy]\n",
    "min(a), max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no. of words in tweets\n",
    "b = [len(i.split()) for i in texts]\n",
    "min(b), max(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = [tokenizer(i, padding='max_length', truncation=True, max_length=max(a)) for i in texts]\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(i.input_ids) for i in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = []\n",
    "for l in annotations:\n",
    "    if l==1:\n",
    "        lbl.append([1,0])\n",
    "    else: \n",
    "        lbl.append([0,1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # \n",
    "labels = torch.tensor(lbl)\n",
    "# labels\n",
    "\n",
    "# add logic if 1 => racist, 0 = non racist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([x.attention_mask for x in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make copy of labels tensor, this will be input_ids\n",
    "# input_ids = labels.detach().clone()\n",
    "\n",
    "\n",
    "# # create random array of floats with equal dims to input_ids\n",
    "# rand = torch.rand(input_ids.shape)\n",
    "\n",
    "# # mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
    "# mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
    "\n",
    "# # loop through each row in input_ids tensor (cannot do in parallel)\n",
    "# for i in range(input_ids.shape[0]):\n",
    "#     # get indices of mask positions from mask array\n",
    "#     selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "#     # mask input_ids\n",
    "#     input_ids[i, selection] = 3  # our custom [MASK] token == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "for i in batch:\n",
    "    input_i = i.input_ids\n",
    "    input_ids.append(input_i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9800\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {'input_ids': input_ids , 'attention_masks': mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "#         for i in batch:\n",
    "#             input_ids = batch['input_ids']\n",
    "        return torch.tensor(self.encodings['input_ids']) .shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: torch.tensor(tensor[i],dtype=torch.int64) for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=len(input_ids), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9800"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification, DebertaConfig\n",
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DebertaConfig()\n",
    "#     vocab_size=50265,\n",
    "#     hidden_size=768,\n",
    "#     num_hidden_layers=12,\n",
    "#     num_attention_heads=12,\n",
    "#     intermediate_size=3072,\n",
    "#     hidden_act='gelu',\n",
    "#     hidden_dropout_prob=0.1,\n",
    "#     attention_probs_dropout_prob=0.1,\n",
    "#     max_position_embeddings=512,\n",
    "#     type_vocab_size=0,\n",
    "#     initializer_range=0.02,\n",
    "#     layer_norm_eps=1e-07,\n",
    "#     relative_attention=False,\n",
    "#     max_relative_positions=-1,\n",
    "#     pad_token_id=0,\n",
    "#     position_biased_input=True,\n",
    "#     pos_att_type=None,\n",
    "#     pooler_dropout=0,\n",
    "#     pooler_hidden_act='gelu',\n",
    "#     **kwargs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DebertaForSequenceClassification(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pytorch_model.bin', mode='rb') as file:\n",
    "#     fileContent = file.read()\n",
    "#     print(fileContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(mode=True)\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe3124777b24cbb9f83db18bca72355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williammdavis/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0658f2e0171e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         outputs = model(input_ids,attention_mask=attention_mask,\n\u001b[0;32m---> 17\u001b[0;31m                         labels=labels) \n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         )\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 11\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_masks'].to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print(input_ids[0])\n",
    "        # process\n",
    "        outputs = model(input_ids,attention_mask=attention_mask,\n",
    "                        labels=labels) \n",
    "        \n",
    "#  \n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = merged_df.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612     walk them dogs if islamist dont like it go bac...\n",
       "1381                                       ima deport you\n",
       "58         nigga why you tripping get your mood right yah\n",
       "1388    this lady is horribly racist busy body who sho...\n",
       "961     man not even thirty seconds after posted that ...\n",
       "                              ...                        \n",
       "26              swear kyra this niggas lips were burgundy\n",
       "130     go lecture to ur fellow jihadis stop wasting m...\n",
       "47          anything telling nigga that hes grown ass man\n",
       "1447    look these immigrants are either bad or me nee...\n",
       "1102    absolutely set up go fund the wall and start d...\n",
       "Name: cleaned_tweet, Length: 100, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = test['cleaned_tweet']\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williammdavis/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# tokenize and output inference \n",
    "all_inference_output_tweets= []\n",
    "for tweet in test_texts:\n",
    "    input_tweets = tokenizer(tweet, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "# #     print(input_tweets)\n",
    "        inference_output_tweets = model(**input_tweets)\n",
    "# #     print(inference_output_tweets)\n",
    "        inference_output_tweets = F.softmax(inference_output_tweets[0]).squeeze()\n",
    "        all_inference_output_tweets.append(inference_output_tweets)\n",
    "    \n",
    "\n",
    "#         print(inference_output_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0838, 0.9162])\n"
     ]
    }
   ],
   "source": [
    "print(inference_output_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID and Model Labeling \n",
    "labeled_tweets = []\n",
    "for i in range(len(test)):\n",
    "    response= {}\n",
    "    index = int(all_inference_output_tweets[i].argmax())\n",
    "    response[\"label\"] = {0: \"0\",1: \"1\"}[index]\n",
    "    response['prob'] = all_inference_output_tweets[i][index]\n",
    "    response['id']= test['cleaned_tweet'].iloc[i]\n",
    "    labeled_tweets.append(response)\n",
    "new_df = pd.DataFrame(labeled_tweets)\n",
    "\n",
    "response[\"prob\"] = {\"not-hateful\": float(inference_output_tweets[0]), \"hateful\": float(inference_output_tweets[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9921)</td>\n",
       "      <td>walk them dogs if islamist dont like it go bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0.8940)</td>\n",
       "      <td>ima deport you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9009)</td>\n",
       "      <td>nigga why you tripping get your mood right yah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0.7799)</td>\n",
       "      <td>this lady is horribly racist busy body who sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.7349)</td>\n",
       "      <td>man not even thirty seconds after posted that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9914)</td>\n",
       "      <td>swear kyra this niggas lips were burgundy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.8858)</td>\n",
       "      <td>go lecture to ur fellow jihadis stop wasting m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9639)</td>\n",
       "      <td>anything telling nigga that hes grown ass man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9760)</td>\n",
       "      <td>look these immigrants are either bad or me nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9162)</td>\n",
       "      <td>absolutely set up go fund the wall and start d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label            prob                                                 id\n",
       "0      1  tensor(0.9921)  walk them dogs if islamist dont like it go bac...\n",
       "1      0  tensor(0.8940)                                     ima deport you\n",
       "2      1  tensor(0.9009)     nigga why you tripping get your mood right yah\n",
       "3      0  tensor(0.7799)  this lady is horribly racist busy body who sho...\n",
       "4      1  tensor(0.7349)  man not even thirty seconds after posted that ...\n",
       "..   ...             ...                                                ...\n",
       "95     1  tensor(0.9914)          swear kyra this niggas lips were burgundy\n",
       "96     1  tensor(0.8858)  go lecture to ur fellow jihadis stop wasting m...\n",
       "97     1  tensor(0.9639)      anything telling nigga that hes grown ass man\n",
       "98     1  tensor(0.9760)  look these immigrants are either bad or me nee...\n",
       "99     1  tensor(0.9162)  absolutely set up go fund the wall and start d...\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.rename(columns={\"id\": \"cleaned_tweet\"})\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>racist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9914)</td>\n",
       "      <td>swear kyra this niggas lips were burgundy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.8858)</td>\n",
       "      <td>go lecture to ur fellow jihadis stop wasting m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9639)</td>\n",
       "      <td>anything telling nigga that hes grown ass man</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9760)</td>\n",
       "      <td>look these immigrants are either bad or me nee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.9162)</td>\n",
       "      <td>absolutely set up go fund the wall and start d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label            prob                                      cleaned_tweet  \\\n",
       "95     1  tensor(0.9914)          swear kyra this niggas lips were burgundy   \n",
       "96     1  tensor(0.8858)  go lecture to ur fellow jihadis stop wasting m...   \n",
       "97     1  tensor(0.9639)      anything telling nigga that hes grown ass man   \n",
       "98     1  tensor(0.9760)  look these immigrants are either bad or me nee...   \n",
       "99     1  tensor(0.9162)  absolutely set up go fund the wall and start d...   \n",
       "\n",
       "    racist  \n",
       "95       0  \n",
       "96       1  \n",
       "97       0  \n",
       "98       1  \n",
       "99       1  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(new_df,test, on = \"cleaned_tweet\", how = \"outer\")\n",
    "merged_df.tail()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612     1\n",
       "1381    1\n",
       "58      0\n",
       "1388    1\n",
       "961     1\n",
       "       ..\n",
       "26      0\n",
       "130     1\n",
       "47      0\n",
       "1447    1\n",
       "1102    1\n",
       "Name: racist, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annts = (test['racist'])\n",
    "annts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1',\n",
       "  'prob': tensor(0.9921),\n",
       "  'id': 'walk them dogs if islamist dont like it go back where you came from'},\n",
       " {'label': '0', 'prob': tensor(0.8940), 'id': 'ima deport you'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9009),\n",
       "  'id': 'nigga why you tripping get your mood right yah'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.7799),\n",
       "  'id': 'this lady is horribly racist busy body who should be heavily fined for not only wasting the policemens time but harassing these poor people just trying to enjoy the nice weather id bet if they were white there would have been no call'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.7349),\n",
       "  'id': 'man not even thirty seconds after posted that saw like four pics of her with no makeup and she defly look like trailer trash who ran into some money and got booty'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.6528),\n",
       "  'id': 'specifically called out two congress people both of whom happen to be women and african americans as illiterate ive asked her repeatedly for examples of there illiteracy she can provide none can any of you help her with this fbr'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9981),\n",
       "  'id': 'hi do you know where can buy mamajuana or where can order it only have one bottle left that bought over year ago on my trip to the dominican republic and im saving this last one for special occasion mamajuana dominicanrepublic'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9992),\n",
       "  'id': 'the only native american in her was on her high school prom night'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.8881),\n",
       "  'id': 'if your such brave soul to stand up for your people and call barbara bush racist have the guts to unprotected your twitter account so we can all enjoy your wisdom'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.7366),\n",
       "  'id': 'most americans have had enough of militant islam but democrats are afraid of alienating fascist leftists aligned with radical islam so ignore problem'},\n",
       " {'label': '1', 'prob': tensor(0.7175), 'id': 'niggas wild'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9425),\n",
       "  'id': 'theres this consistent narrative that asian americans are the model minoritys but also the forever foreigner'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.8453),\n",
       "  'id': 'this was said by democrat president during the civilrights marches of the that the liberal democrats claim they supported the socialist liberals have been lying to blacks mexicans and nativeamericans for the last years stopthednc vote independent'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9245),\n",
       "  'id': 'what type of real street nigga leave his patna by himself'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9877),\n",
       "  'id': 'cuz undocumented children is whats wrong with our school systems yall backwards fr'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9913),\n",
       "  'id': 'take nigga bitch then pass her to the clique'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9364),\n",
       "  'id': 'listened to white rapper say nigga once so its okay to say it guys youd think this guy is kidding but he is not'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9860),\n",
       "  'id': 'this makes total sense it is time to build that wall and if california can not straighten out and fly right then we need to build wall around california make america safe again'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9401),\n",
       "  'id': 'see you too young to understand its song go away mexican'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9981),\n",
       "  'id': 'thank you for following me and god bless america and let build the border wall now'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9887),\n",
       "  'id': 'told my cousin get me an icee because im in pain from my tonsils swelling up and this nigga says done suck the wrong dick huhh bitchhhhh'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9914),\n",
       "  'id': 'not eating just sniffing this new breed of niggas yall can tell this person born in'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9179),\n",
       "  'id': 'under seraphim authority vol election day ghetto gov usa by various artists'},\n",
       " {'label': '0', 'prob': tensor(0.9998), 'id': 'thai food sounds lovely rn'},\n",
       " {'label': '0', 'prob': tensor(0.9238), 'id': 'deport them all'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.7490),\n",
       "  'id': 'want to make it perfectly clear do not support any illegal immigration from norway any illegal immigration is not acceptable from any culture hithole or not'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9919),\n",
       "  'id': 'ouuu that white boy scrambling'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.8578),\n",
       "  'id': 'nah thats the dominicans puerto ricans be like'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.5687),\n",
       "  'id': 'today on what grinds my gears how hilarious ppl think it is that look like an asian mexican mutt oh boy never heard that before ha ha ha ya got me im not just white'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8435),\n",
       "  'id': 've been kinda silent the last day or so as ve tried to process this ongoing criminal enterprise that we are forced to call government coming out of it now said after jabba first comments about illegal aliens being rapists he was hitler reincarnated was right'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9764),\n",
       "  'id': 'how about they go back where they came from'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9985),\n",
       "  'id': 'azerbaijan is great friend of the jewish people executive vice chairman of of major american jewish organizations malcolmhoenlein'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9586),\n",
       "  'id': 'you gonna pimp her gas just for her to get in the car with her nigga foolish'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9718),\n",
       "  'id': 'are you an indian prince or soemthjng'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9057),\n",
       "  'id': 'may suggest friday night movie starts soon on trump tv wagthedog protectmueller assad assadgenocide syria war congress'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9957),\n",
       "  'id': 'these black ass nasty teeth ass hoes be feeling gassed up cause these broke ass niggas stay gassing'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9813),\n",
       "  'id': 'didnt loyola put kool aid in the cafeteria for black history month cuz'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.6711),\n",
       "  'id': 'my white privilege ass boss is rly trying to argue with me that reverse racism is real and white ppl are the ones being oppressed get me the fuck out of here'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8789),\n",
       "  'id': 'real nigga holiday ahaa happy day mama'},\n",
       " {'label': '1', 'prob': tensor(0.9855), 'id': 'ya niggas aint ya niggas'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.7877),\n",
       "  'id': 'have japanese akita who afraid of cardboard boxes he has been trained to go outside or away from children and anyone we tell him to leave be so think it is training issue'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8004),\n",
       "  'id': 'if speak to you and you dont speak back ima automatically think you stole something out the store or ya racist'},\n",
       " {'label': '1', 'prob': tensor(0.9990), 'id': 'niggas dont want you back'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9681),\n",
       "  'id': 'ill never sit in nigga face without having my own money lol every date ive been on had my card or cash these niggas be thinking were broke as fuck'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9098),\n",
       "  'id': 'she bout her money okay ling ling'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.6808),\n",
       "  'id': 'ahed tamimi failed to show remorse in israeli court because shes sunni muslim she doesnt know the difference between jewish and catholic guilt'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9855),\n",
       "  'id': 'these niggas extra but wont none of them see me this why you cant play about sports on social media cuz these niggas get offended and start getting disrespectful at the mouth im sure cant none of them hold me in basketball and im sure know more basketball then they do'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9645),\n",
       "  'id': 'somebody told me changed and was like tf how and this nigga said cause you got skinny and now yo head big as hell'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9884),\n",
       "  'id': 'one thing dont do is kiss ass we wont ever speak again wrong nigga fr'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9578),\n",
       "  'id': 'my mans talking about isnt weird how we keep playing this game lmao nigga what you added me on snap boy its been years plz lmao stop god testing me see you'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9282),\n",
       "  'id': 'brave aaaand now native americans are offended'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9840),\n",
       "  'id': 'wth she know damn well she dont want no hood nigga she look like she daddys girl lika mf she wylin'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.5910),\n",
       "  'id': 'is not invading syria or going after regime change to replace western democracy as other neocons have done he is punishing madman with one night flawless attack warning assad if he uses chemical weapons again trumpsupporter doing great job little help'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.6563),\n",
       "  'id': 'stop lying take the test then prove to us all that your native american even the cherokee nation says your liar shame on you gone on prove it'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9924),\n",
       "  'id': 'fav this and ill guess your age height ethnicity nigger relationship status taken body count idk like white girls nasty hoe'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8980),\n",
       "  'id': 'the big sick is the muslim version of guess who coming to dinner easy on liberal white audian eyes films that fit to see'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9979),\n",
       "  'id': 'please have potus be informed that we need an executive order to stop imagration to our country these immigrants are bringing over deadly diseases ppl should have health clearance we will have an epidemic on are hands maybe more'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9995),\n",
       "  'id': 'my property manager really shows they dont give fuck smh but if my rent was late them fucking jews would care then'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8906),\n",
       "  'id': 'obama divided this country more than anyone where was lebrons huge mouth then curry nothing blacks are soo prejudiced get over yourself you we arent listening to your prejudiced opinions sthu'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9985),\n",
       "  'id': 'if black people we saiyans and frieza called them apes would that be racist'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.5372),\n",
       "  'id': 'got type waab skinny nigga with tattoos love emmmmmmm'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.6321),\n",
       "  'id': 'let her protest in muslim country'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9905),\n",
       "  'id': 'hi have you heard of the gcb exam have project am doing for my cultural geography class interviewing an american immigrant map and he said the kids at his school on the english track took gcb exam'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9817),\n",
       "  'id': 'take your lies and go away hate jews muslin man'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9979),\n",
       "  'id': 'check out what selling on letgo yellow gold native american inspired ring'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9478),\n",
       "  'id': 'liberal brand is antisemitic or anti zionist if your jewish'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9542),\n",
       "  'id': 'all white girls twerk the same sweaa'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9521),\n",
       "  'id': 'how about politicians shut up and do their jobs instead of attacking the black athlete for having an opinion its classic racism trying to oppress the opinion of someone who looks different or thinks different than them'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.8511),\n",
       "  'id': 'yea im about to find me white man with great credit'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9555),\n",
       "  'id': 'monica you start with cultural stop sucking cocks never met this kind in any theater museum or book reading probably you were in those bookstore in your knees such nice jewish girl aren you'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8966),\n",
       "  'id': 'colin kap took knee lost his job do you consider all black athletes that did not kneee and lose their jobs house niggas too kanye is actually standing out and showing it okay to express your own opinions and views'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.6572),\n",
       "  'id': 'mayors of sanctuary cities put interest of illegal aliens over legal immigrants and american citizens they support the criminals over americans its ridiculous'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9448),\n",
       "  'id': 'ling ling took hours to do my nails but she finessed'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9573),\n",
       "  'id': 'wheres our candy nigga in hawaii lmfao aint buying yall shieeeet'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9634),\n",
       "  'id': 'for those of you salivating over the most recent claims that slavery was choice by the worse uncle tom of this century the delusional kardashian wannabe demagogue here something else to add to your hypocritical racist bigoted beliefs tick tock bitches'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9250),\n",
       "  'id': 'it is just statistically correct and you are just closed minded bigoted racist'},\n",
       " {'label': '1', 'prob': tensor(0.8884), 'id': 'you my nigga lmao'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9991),\n",
       "  'id': 'cause be killin these niggas out here'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9909),\n",
       "  'id': 'how tryna get me back if act like hoe out on the weekends and text my niggas'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.8608),\n",
       "  'id': 'john my parents were immigrants followed all the rules to come to the waited their turn and fully complied with all immigration laws assimilated fully and worked hard all their lives not fair to give preference for citizenship to illegal aliens'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9989),\n",
       "  'id': 'mann niggas really weird out here dont get it'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9992),\n",
       "  'id': 'scarlett johansson must really enjoy taking roles from minorities'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9893),\n",
       "  'id': 'hbcus are producing top tier african americans hbcus have been changing lives since reconstruction thank you for this akil kamau cacny'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.6631),\n",
       "  'id': 'we joke about it now survived barrens chat ha ha ha barrens chat was racism sexism homophobia shockjock type bullshit then barrens chat became trade chat lfr now its twitter reddit im glad yinz have had great guild communities but it isnt what wow is'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9999),\n",
       "  'id': 'mexicans getting separated at the border via'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9986),\n",
       "  'id': 'just do not understand caucasians'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9557),\n",
       "  'id': 'girl you are pathetic to continue to stand by this racist corrupt fraud asshole illegitimate piece of crap you day of reckoning will come'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.8750),\n",
       "  'id': 'first bbqbecky now permitpatty cant with these racists people'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9560),\n",
       "  'id': 'but you bro you have lot of niggas against you and that just goes to show you in actuality the wack nigga you are'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9854),\n",
       "  'id': 'lives in lilly white neighborhood and doesnt have to worry about being raped by muslims unfortunately most europeans dont have or cant afford the protection level she has'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.6492),\n",
       "  'id': 'best thing to happen to spain was the islamic purge in neverforget'},\n",
       " {'label': '0',\n",
       "  'prob': tensor(0.9901),\n",
       "  'id': 'yu can doooo it in my mexican voice'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9584),\n",
       "  'id': 'girl dont leave yo nigga every couple have they bumps but continue calling me when that nigga go ta acting up'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.7753),\n",
       "  'id': 'its like reward if she can trust the guy or she know that hes not going to do nothing shady then itll be no problem but if she fw aint shit nigga then she wont feel comfortable doing that shell be to insecure'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8488),\n",
       "  'id': 'my coworkers said look not asian dont raisin know'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9914),\n",
       "  'id': 'swear kyra this niggas lips were burgundy'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.8858),\n",
       "  'id': 'go lecture to ur fellow jihadis stop wasting my time with your lies'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9639),\n",
       "  'id': 'anything telling nigga that hes grown ass man'},\n",
       " {'label': '1',\n",
       "  'prob': tensor(0.9760),\n",
       "  'id': 'look these immigrants are either bad or me need another narrative but to keep moving the line that its ok we voting base needs immigrants its ok this is hypocrisy at it best yet and his minions go along buffalos over cliff'},\n",
       " {'label': '1',\n",
       "  'prob': {'not-hateful': 0.08381436765193939, 'hateful': 0.9161855578422546},\n",
       "  'id': 'absolutely set up go fund the wall and start deportation of illegals'}]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = [a_dict['label'] for a_dict in labeled_tweets]\n",
    "y_pred = np.array(y_pred, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "annts = np.array(annts, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49559845559845556, 0.49, 0.48148193136957174, None)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(annts, y_pred, average='macro')\n",
    "precision_recall_fscore_support(annts, y_pred, average='micro')\n",
    "precision_recall_fscore_support(annts, y_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "new= \"I love heran\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tweets = tokenizer(new, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,  282, 1610, 3418,  154,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_output = model(**input_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8463, -3.3395]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williammdavis/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'not-hateful',\n",
       " 'prob': tensor(0.9979, grad_fn=<SelectBackward>),\n",
       " 'id': 'nbe wilding'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = {}\n",
    "index = int(inference_output_tweets.argmax())\n",
    "response[\"label\"] = {0: \"not-hateful\",1: \"hateful\"}[index]\n",
    "response['prob'] = inference_output_tweets[index]\n",
    "response['id']= new\n",
    "# labeled_tweets.append (response)\n",
    "# new_df = pd.DataFrame(response, index=1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenize and output inference \n",
    "# all_inference_output_tweets= []\n",
    "# for tweet in texts:\n",
    "#     input_tweets = tokenizer(tweet, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "# # #     print(input_tweets)\n",
    "#         inference_output_tweets = model(**input_tweets)\n",
    "# # #     print(inference_output_tweets)\n",
    "#         inference_output_tweets = F.softmax(inference_output_tweets[0]).squeeze()\n",
    "#         all_inference_output_tweets.append(inference_output_tweets)\n",
    "    \n",
    "\n",
    "# #         print(inference_output_tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ID and Model Labeling \n",
    "# labeled_tweets = []\n",
    "# for i in range(len(merged_df)):\n",
    "#     response= {}\n",
    "#     index = int(all_inference_output_tweets[i].argmax())\n",
    "#     response[\"label\"] = {0: \"not-hateful\",1: \"hateful\"}[index]\n",
    "#     response['prob'] = all_inference_output_tweets[i][index]\n",
    "#     response['id']= sample_df['cleaned_tweet'].iloc[i]\n",
    "#     labeled_tweets.append(response)\n",
    "# new_df = pd.DataFrame(labeled_tweets)\n",
    "\n",
    "# response[\"prob\"] = {\"not-hateful\": float(inference_output_tweets[0]), \"hateful\": float(inference_output_tweets[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hateful'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.rename(columns={\"id\": \"cleaned_tweet\"})\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(new_df,sample_df, on = \"cleaned_tweet\", how = \"outer\")\n",
    "# merged_df.tail()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_hateful = merged_df[(merged_df['racist']==1) & (merged_df['label']=='hateful')]\n",
    "# print(len(matched_hateful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umatched_hateful = merged_df[(merged_df['racist']==0) & (merged_df['label']=='hateful')]\n",
    "# print(len(umatched_hateful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_not_hateful = merged_df[(merged_df['racist']==0) & (merged_df['label']=='not-hateful')]\n",
    "# print(len(matched_not_hateful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umatched_not_hateful = merged_df[(merged_df['racist']==1) & (merged_df['label']=='not-hateful')]\n",
    "# print(len(umatched_not_hateful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_hateful = merged_df[(merged_df['hate_speech']==1) & (merged_df['label']=='not-hateful')]\n",
    "# text = not_hateful['tweet']\n",
    "# text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv('merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import simpletransformers\n",
    "# from simpletransformers.classification import (\n",
    "#     ClassificationModel, ClassificationArgs\n",
    "# )\n",
    "# import pandas as pd\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# # Preparing train data\n",
    "# train_data = [\n",
    "#     [\n",
    "#         \" I hate black people\",\n",
    "#         \"Send Asians Back\",\n",
    "#         1,\n",
    "#     ],\n",
    "#     [\n",
    "#         \"I do not feel like working\",\n",
    "#         \"Love Ethiopian Food\",\n",
    "#         0,\n",
    "#     ],\n",
    "# ]\n",
    "# train_df = pd.DataFrame(train_data)\n",
    "# train_df.columns = [\"text_a\", \"text_b\", \"labels\"]\n",
    "\n",
    "# # Preparing eval data\n",
    "# eval_data = [\n",
    "#     [\n",
    "#         \"Black Lives Do not matter \",\n",
    "#         \"Build the wall\",\n",
    "#         1,\n",
    "#     ],\n",
    "#     [\n",
    "#         \"Merry was the king of Rohan\",\n",
    "#         \"Legolas was taller than Gimli\",\n",
    "#         0,\n",
    "#     ],\n",
    "# ]\n",
    "# eval_df = pd.DataFrame(eval_data)\n",
    "# eval_df.columns = [\"text_a\", \"text_b\", \"labels\"]\n",
    "\n",
    "# # Optional model configuration\n",
    "# model_args = ClassificationArgs(num_train_epochs=1)\n",
    "\n",
    "# # Create a ClassificationModel\n",
    "# model = ClassificationModel(\"deberta\", \"microsoft/deberta-base\", use_cuda=False)\n",
    "\n",
    "# # Train the model\n",
    "# model.train_model(train_df, output_dir='outputs' )\n",
    "\n",
    "# # Evaluate the model\n",
    "# result, model_outputs, wrong_predictions = model.eval_model(\n",
    "#     eval_df\n",
    "# )\n",
    "\n",
    "# # Make predictions with the model\n",
    "# predictions, raw_outputs = model.predict(\n",
    "#     [\n",
    "#         [\n",
    "#             \"Fuck covid asians\",\n",
    "#             \"Muslims should not come\",\n",
    "#         ]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # from models.multi_choice import MultiChoiceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
